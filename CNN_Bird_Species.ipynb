{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e265c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "from PIL import Image\n",
    "# to install pytorch, follow instructions on https://pytorch.org/get-started/locally/\n",
    "# if CUDA is installed, this should allow GPU training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# -> pip install torchsummary\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3999fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Path to expanded dataset\n",
    "dataset_dir = r\"C:\\Users\\antoi\\Documents\\BirdSpeciesRecognitionProject\\DATA\"\n",
    "\n",
    "# Step 1: Load images and labels from the dataset directory\n",
    "def load_images_from_directory(directory, target_size=(256, 256)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    # Get sorted list of class names (subdirectories), ensures consistent label order\n",
    "    class_names = sorted(os.listdir(directory))\n",
    "    # Map class names to integer indices\n",
    "    class_to_index = {name: idx for idx, name in enumerate(class_names)}\n",
    "\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(directory, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue  # Skip if not a directory\n",
    "\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            try:\n",
    "                # Open image, convert to RGB, resize to target size\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                img = img.resize(target_size)\n",
    "                images.append(np.array(img))  # Convert image to numpy array\n",
    "                labels.append(class_to_index[class_name])  # Store label as integer\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {img_path}: {e}\")  # Skip unreadable images\n",
    "\n",
    "    return np.array(images), np.array(labels), class_to_index\n",
    "\n",
    "# Load all images and labels, and get the label map\n",
    "images, labels, label_map = load_images_from_directory(dataset_dir)\n",
    "\n",
    "# Step 2: Split into training and validation sets (80% train, 20% validation)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    images, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Transpose images to match PyTorch's expected input shape: (N, C, H, W)\n",
    "train_images = train_images.transpose(0, 3, 1, 2)\n",
    "val_images = val_images.transpose(0, 3, 1, 2)\n",
    "\n",
    "# Print dataset summary\n",
    "print(\"Dataset split complete:\")\n",
    "print(f\"Train set: {len(train_images)} images\")\n",
    "print(f\"Validation set: {len(val_images)} images\")\n",
    "print(\"Label map:\", label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce897c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# How many augmented versions per image you want:\n",
    "copies_per_image = 10  # You can change this to any number you like\n",
    "\n",
    "# Define a set of data augmentation transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(20, fill=(255, 255, 255)),  # Random rotation with white border\n",
    "    transforms.RandomAffine(20, translate=(0.2, 0.2), fill=(255, 255, 255)),  # Random shifts with white border\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flips\n",
    "    transforms.ColorJitter(brightness=(0.8, 1.2)),  # Random brightness adjustment\n",
    "    transforms.ToTensor()  # Convert image to tensor\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Apply the transformations to the training dataset\n",
    "class AugmentedDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "\n",
    "        # If it's a PyTorch tensor, convert to NumPy\n",
    "        if torch.is_tensor(image):\n",
    "            image = image.numpy()\n",
    "\n",
    "        # If shape is (C, H, W), convert to (H, W, C)\n",
    "        if image.ndim == 3 and image.shape[0] == 3:\n",
    "            image = np.transpose(image, (1, 2, 0))  # Convert from CHW to HWC\n",
    "\n",
    "        # Fix: Handle grayscale or RGB image formats\n",
    "        if image.ndim == 3 and image.shape[0] == 1:\n",
    "            image = np.squeeze(image, axis=0)  # (1, H, W) -> (H, W)\n",
    "        elif image.ndim == 3 and image.shape[2] == 1:\n",
    "            image = np.squeeze(image, axis=2)  # (H, W, 1) -> (H, W)\n",
    "\n",
    "        if image.ndim == 2:  # Grayscale\n",
    "            pil_image = Image.fromarray(image.astype('uint8'), mode='L').convert('RGB')\n",
    "        elif image.ndim == 3 and image.shape[2] == 3:  # RGB\n",
    "            pil_image = Image.fromarray(image.astype('uint8'), mode='RGB')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported image shape: {image.shape}\")\n",
    "\n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(pil_image)\n",
    "        else:\n",
    "            image_tensor = transforms.ToTensor()(pil_image)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image_tensor, label\n",
    "\n",
    "\n",
    "# Create the augmented dataset\n",
    "#augmented_dataset = AugmentedDataset(train_images, train_labels, transform=transform)\n",
    "\n",
    "# Create N versions of the same dataset (each with random transforms)\n",
    "augmented_dataset_repeated = ConcatDataset([AugmentedDataset(train_images, train_labels, transform=transform) \n",
    "                                             for _ in range(copies_per_image)])\n",
    "\n",
    "# Create a DataLoader to efficiently load batches of augmented training data during model training\n",
    "augmented_loader = DataLoader(augmented_dataset_repeated, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2971b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Convert validation images to torch tensor, normalize to [0, 1] by dividing by 255\n",
    "validation_images_tensor = torch.from_numpy(val_images).float().div(255)\n",
    "\n",
    "# Convert validation labels to torch tensor of type long (for classification)\n",
    "validation_labels_tensor = torch.from_numpy(val_labels).long()\n",
    "\n",
    "# Create a TensorDataset for validation data\n",
    "validation_dataset = TensorDataset(validation_images_tensor, validation_labels_tensor)\n",
    "\n",
    "# Create a DataLoader for the validation dataset (no shuffling)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0213df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch of augmented images\n",
    "augmented_images, _ = next(iter(augmented_loader))\n",
    "\n",
    "# Convert the images to numpy format for visualization\n",
    "augmented_images = augmented_images.permute(0, 2, 3, 1).numpy()\n",
    "\n",
    "# Create a grid to display the images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Display each image in the grid\n",
    "for img, ax in zip(augmented_images, axes):\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')  # Hide axes\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8ecf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdSpeciesCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BirdSpeciesCNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=0)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=0)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=0)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout3 = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=0)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout4 = nn.Dropout(p=0.2)\n",
    "\n",
    "        # New layer\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=0)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout5 = nn.Dropout(p=0.2)\n",
    "\n",
    "        # Dynamically calculate the flattened size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 256, 256)  # Adjust input size if needed\n",
    "            x = self.pool1(F.relu(self.conv1(dummy_input)))\n",
    "            x = self.pool2(F.relu(self.conv2(x)))\n",
    "            x = self.pool3(F.relu(self.conv3(x)))\n",
    "            x = self.pool4(F.relu(self.conv4(x)))\n",
    "            x = self.pool5(F.relu(self.conv5(x)))\n",
    "            self.flatten_size = x.numel()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 1024)\n",
    "        self.dropout_fc = nn.Dropout(p=0.3)\n",
    "        self.fc2 = nn.Linear(1024, 21)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.pool3(F.relu(self.conv3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.pool4(F.relu(self.conv4(x)))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.pool5(F.relu(self.conv5(x)))  # Include the new layer\n",
    "        x = self.dropout5(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Example of model instantiation\n",
    "model = BirdSpeciesCNNModel()\n",
    "print(model)\n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470f7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "# IMPORTANT: You must also install the Graphviz system package and add its bin folder to your PATH.\n",
    "# Download from: https://graphviz.gitlab.io/download/\n",
    "# After installation, add the Graphviz 'bin' directory (e.g., C:\\Program Files\\Graphviz\\bin) to your system PATH.\n",
    "\n",
    "# Create a dummy input tensor with the same shape as the model expects\n",
    "dummy_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "\n",
    "# Get the model output\n",
    "model.eval()\n",
    "output = model(dummy_input)\n",
    "\n",
    "# Visualize the model graph\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "dot.format = 'png'\n",
    "dot.render('cnn_model_graph.png', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79655d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size=(3, 256, 256))  # Adjust input size if needed\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b1c1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = 32  # Adjusted batch size for augmented dataset\n",
    "epochs = 20  # Define the number of epochs for training\n",
    "\n",
    "# Recreate the dataloader with the new batch size\n",
    "# Use augmented_dataset_repeated for augmentation as in previous cells\n",
    "augmented_loader = DataLoader(augmented_dataset_repeated, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize lists to store loss and accuracy history\n",
    "loss_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "# Set up real-time plotting\n",
    "plt.ion()  # Turn on interactive mode\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax[0].set_title(\"Loss Over Epochs\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].set_title(\"Accuracy Over Epochs\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy (%)\")\n",
    "\n",
    "loss_line, = ax[0].plot([], [], label=\"Loss\", color=\"blue\")\n",
    "acc_line, = ax[1].plot([], [], label=\"Accuracy\", color=\"green\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in augmented_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(augmented_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "    loss_history.append(epoch_loss)\n",
    "    accuracy_history.append(epoch_accuracy)\n",
    "\n",
    "    # Update the plots in real time\n",
    "    loss_line.set_data(range(1, epoch + 2), loss_history)\n",
    "    acc_line.set_data(range(1, epoch + 2), accuracy_history)\n",
    "    ax[0].relim()\n",
    "    ax[0].autoscale_view()\n",
    "    ax[1].relim()\n",
    "    ax[1].autoscale_view()\n",
    "    plt.draw()\n",
    "    plt.pause(0.1)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%, Images processed: {total}\")\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3d729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save accuracy and loss graphs over epochs\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax[0].plot(range(1, len(loss_history) + 1), loss_history, color=\"blue\", label=\"Loss\")\n",
    "ax[0].set_title(\"Loss Over Epochs\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(range(1, len(accuracy_history) + 1), accuracy_history, color=\"green\", label=\"Accuracy\")\n",
    "ax[1].set_title(\"Accuracy Over Epochs\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Accuracy (%)\")\n",
    "ax[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"accuracy_loss_over_epochs.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6442fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradients for evaluation\n",
    "    for test_X, test_y in validation_dataloader:\n",
    "        # Move data to GPU\n",
    "        test_X, test_y = test_X.to(device), test_y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        test_outputs = model(test_X)\n",
    "        \n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(test_outputs, 1)  # Get the class with highest probability\n",
    "        \n",
    "        # Update total and correct predictions\n",
    "        correct += (predicted == test_y).sum().item()\n",
    "        total += test_y.size(0)\n",
    "\n",
    "# Compute final accuracy across all batches\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cde2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the image\n",
    "def preprocess_image(image_path, target_size=(256, 256)):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = image.resize(target_size)\n",
    "    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image_tensor\n",
    "\n",
    "# Path to the test image\n",
    "test_image_path = r\"C:\\Users\\antoi\\Documents\\BirdSpeciesRecognitionProject\\TestPics\\téléchargement (3).jpg\"\n",
    "\n",
    "# Preprocess the image\n",
    "test_image_tensor = preprocess_image(test_image_path).to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = model(test_image_tensor)\n",
    "    predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "# Map the predicted class index to the class name\n",
    "predicted_label = list(label_map.keys())[list(label_map.values()).index(predicted_class)]\n",
    "print(f\"Predicted class: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model's state_dict\n",
    "model_save_path = \"bird_species_cnn_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
